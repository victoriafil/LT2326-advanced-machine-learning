{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7bfe9e-165c-41ff-a8d4-6745f231f53a",
      "metadata": {
        "id": "cb7bfe9e-165c-41ff-a8d4-6745f231f53a"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "#!pip install transformers\n",
        "#!pip install evaluate\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee4bb3ee-dfc6-4ce9-ab62-782dbe1700b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee4bb3ee-dfc6-4ce9-ab62-782dbe1700b3",
        "outputId": "b0391d2e-a29f-45fb-eee5-34deaa8f519e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=382x512>,\n",
              " 'label': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "food = load_dataset(\"food101\", split=\"train[:5000]\")\n",
        "#split dataset\n",
        "food = food.train_test_split(test_size=0.2)\n",
        "#example\n",
        "food[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ddcdab-9488-47e1-8b0c-bdb87def250e",
      "metadata": {
        "id": "33ddcdab-9488-47e1-8b0c-bdb87def250e"
      },
      "outputs": [],
      "source": [
        "labels = food[\"train\"].features[\"label\"].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0100f466-eea4-4379-bb7b-8e6b677c1606",
      "metadata": {
        "id": "0100f466-eea4-4379-bb7b-8e6b677c1606"
      },
      "outputs": [],
      "source": [
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652cb97b-e261-4ffb-9b02-b710a29cf73a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "652cb97b-e261-4ffb-9b02-b710a29cf73a",
        "outputId": "1c3796d3-68ee-4613-8f59-12e8cc9d1e3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prime_rib'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "id2label[str(79)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b916050-fd4b-4823-aa6d-2d06a80fd35b",
      "metadata": {
        "id": "5b916050-fd4b-4823-aa6d-2d06a80fd35b"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90fd9de9-fcd9-457b-bc41-ed9a36b5d294",
      "metadata": {
        "id": "90fd9de9-fcd9-457b-bc41-ed9a36b5d294"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041375ea-cc60-4bb6-9b1a-3c10aa93fa83",
      "metadata": {
        "id": "041375ea-cc60-4bb6-9b1a-3c10aa93fa83"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b53007ae-c107-4bc8-9a33-f45113a72f2d",
      "metadata": {
        "id": "b53007ae-c107-4bc8-9a33-f45113a72f2d"
      },
      "outputs": [],
      "source": [
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "size = (\n",
        "    image_processor.size[\"shortest_edge\"]\n",
        "    if \"shortest_edge\" in image_processor.size\n",
        "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        ")\n",
        "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb613af-8aba-4d7a-afcb-02d8592fa899",
      "metadata": {
        "id": "dbb613af-8aba-4d7a-afcb-02d8592fa899"
      },
      "outputs": [],
      "source": [
        "def transforms(examples):\n",
        "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "    del examples[\"image\"]\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c79662a-3bac-45c3-bec1-fa88a57905ed",
      "metadata": {
        "id": "5c79662a-3bac-45c3-bec1-fa88a57905ed"
      },
      "outputs": [],
      "source": [
        "food = food.with_transform(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fdfe565-2c74-48fc-ac4d-3f55071eaff7",
      "metadata": {
        "id": "2fdfe565-2c74-48fc-ac4d-3f55071eaff7"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b8501cd-cc6b-4553-8d7c-56dc3fcea708",
      "metadata": {
        "id": "8b8501cd-cc6b-4553-8d7c-56dc3fcea708"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f4f085e-9a1d-4513-8400-7ae4b268310d",
      "metadata": {
        "id": "5f4f085e-9a1d-4513-8400-7ae4b268310d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07f49a1-b449-4916-8cd8-9f4f57f58edf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b07f49a1-b449-4916-8cd8-9f4f57f58edf",
        "outputId": "53de2528-f622-4640-d2d8-0c4103a86894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5618ab-875c-4962-a970-99296bb31b1c",
      "metadata": {
        "id": "dc5618ab-875c-4962-a970-99296bb31b1c"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers[torch]\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"food_model\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f51a823-b9eb-4bb4-bf8a-13d19b8ad16a",
      "metadata": {
        "id": "1f51a823-b9eb-4bb4-bf8a-13d19b8ad16a"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=food[\"train\"],\n",
        "    eval_dataset=food[\"test\"],\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a907a0-78f5-4d12-b588-82d8c68ece94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "87a907a0-78f5-4d12-b588-82d8c68ece94",
        "outputId": "601bea00-3fa3-404c-e56e-0e1fc81561ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [186/186 08:41, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.711800</td>\n",
              "      <td>2.525361</td>\n",
              "      <td>0.829000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.639600</td>\n",
              "      <td>1.609706</td>\n",
              "      <td>0.896000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=186, training_loss=2.4524936983662267, metrics={'train_runtime': 527.1184, 'train_samples_per_second': 22.765, 'train_steps_per_second': 0.353, 'total_flos': 9.232831524962304e+17, 'train_loss': 2.4524936983662267, 'epoch': 2.98})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(output_dir=\"best_food\")"
      ],
      "metadata": {
        "id": "DdTECNNXO62t"
      },
      "id": "DdTECNNXO62t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4d257d-8b70-4363-b88b-4c51ba94fe5d",
      "metadata": {
        "id": "0f4d257d-8b70-4363-b88b-4c51ba94fe5d"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n",
        "image = ds[\"image\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca731a51-ea44-4971-9170-40eafed48c3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca731a51-ea44-4971-9170-40eafed48c3e",
        "outputId": "e0cd30e6-ff99-491c-e7e3-a1a0ef90f9e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.2840491533279419, 'label': 'beignets'},\n",
              " {'score': 0.019540566951036453, 'label': 'ramen'},\n",
              " {'score': 0.01433509774506092, 'label': 'bruschetta'},\n",
              " {'score': 0.01351894997060299, 'label': 'prime_rib'},\n",
              " {'score': 0.013120424933731556, 'label': 'hamburger'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"image-classification\", model=\"best_food\")\n",
        "classifier(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1162bc-38f7-4e0e-aa76-8a40bd45d9d1",
      "metadata": {
        "id": "9d1162bc-38f7-4e0e-aa76-8a40bd45d9d1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "import torch\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"best_food\")\n",
        "inputs = image_processor(image, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3b23928-50d9-4ce1-8429-eb389468dd45",
      "metadata": {
        "id": "d3b23928-50d9-4ce1-8429-eb389468dd45"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\"best_food\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label = logits.argmax(-1).item()\n",
        "model.config.id2label[predicted_label]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b3Sc_FDlp_XI",
        "outputId": "14964556-49e0-4791-c535-42f34718c37e"
      },
      "id": "b3Sc_FDlp_XI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'beignets'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjKB3EXcqHkv"
      },
      "id": "YjKB3EXcqHkv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
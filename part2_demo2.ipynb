{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1e222e5-be08-4b7c-a98a-e5c814c6a836",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:00.096245Z",
          "iopub.status.busy": "2023-11-07T18:52:00.096057Z",
          "iopub.status.idle": "2023-11-07T18:52:02.041954Z",
          "shell.execute_reply": "2023-11-07T18:52:02.041249Z",
          "shell.execute_reply.started": "2023-11-07T18:52:00.096226Z"
        },
        "id": "c1e222e5-be08-4b7c-a98a-e5c814c6a836",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "#!pip install datasets\n",
        "#!pip install evaluate\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertModel, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, AutoModelForMaskedLM\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "from datasets import Dataset as ds\n",
        "from datasets import DatasetDict, load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "device = torch.device('cuda:2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a700a3c-4130-4bea-93e3-09f85ab7e5f7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:02.044824Z",
          "iopub.status.busy": "2023-11-07T18:52:02.044068Z",
          "iopub.status.idle": "2023-11-07T18:52:02.050057Z",
          "shell.execute_reply": "2023-11-07T18:52:02.049469Z",
          "shell.execute_reply.started": "2023-11-07T18:52:02.044789Z"
        },
        "id": "4a700a3c-4130-4bea-93e3-09f85ab7e5f7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def read_chinese_data(inputfilename):\n",
        "    with open(inputfilename, \"r\") as inputfile:\n",
        "        sentences = []\n",
        "        collection_words = []\n",
        "        collection_labels = []\n",
        "        for line in inputfile:\n",
        "            if line[0] == '#':\n",
        "                continue\n",
        "            columns = line.split()\n",
        "            #print(words)\n",
        "            if columns == []:\n",
        "                sentences.append((''.join(collection_words), collection_labels))\n",
        "                collection_words = []\n",
        "                collection_labels = []\n",
        "                continue\n",
        "            collection_words.append(columns[1])\n",
        "            collection_labels += [1] + ([0] * (len(columns[1]) - 1))\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3b632a85-c96c-412e-ae1a-1698b6c65e43",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:02.051180Z",
          "iopub.status.busy": "2023-11-07T18:52:02.050903Z",
          "iopub.status.idle": "2023-11-07T18:52:02.219571Z",
          "shell.execute_reply": "2023-11-07T18:52:02.219066Z",
          "shell.execute_reply.started": "2023-11-07T18:52:02.051151Z"
        },
        "id": "3b632a85-c96c-412e-ae1a-1698b6c65e43",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_sentences = read_chinese_data('zh_gsd-ud-train.conllu')\n",
        "test_sentences = read_chinese_data('zh_gsd-ud-test.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "56bcb14f-d2d9-415d-b435-485b1c367a02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:02.221117Z",
          "iopub.status.busy": "2023-11-07T18:52:02.220600Z",
          "iopub.status.idle": "2023-11-07T18:52:02.670221Z",
          "shell.execute_reply": "2023-11-07T18:52:02.669541Z",
          "shell.execute_reply.started": "2023-11-07T18:52:02.221086Z"
        },
        "id": "56bcb14f-d2d9-415d-b435-485b1c367a02",
        "outputId": "ac47740c-b1d2-400d-ee9d-4f24ab60445b",
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('看似簡單，只是二選一做決擇，但其實他們代表的是你周遭的親朋好友，試著給你不同的意見，但追根究底，最後決定的還是自己。',\n",
              "  [1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1]),\n",
              " ('其便當都是買來的，就算加熱也是由媽媽負責（後來揭曉其實是避免帶來厄運），父親則在電視台上班。',\n",
              "  [1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1]),\n",
              " ('這次遊行最大的特色，在於越來越多年輕人上街遊行，而且當中不乏行動激烈的躁少年。',\n",
              "  [1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1]),\n",
              " ('懷孕期為421至457日。', [1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]),\n",
              " ('婷婷向昏迷中的婆婆訴說，為什麼生活會與她想像的不一樣。',\n",
              "  [1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   0,\n",
              "   1])]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_sentences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bca54146-fa57-4132-81ee-d09ee6e8c8b8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:02.671311Z",
          "iopub.status.busy": "2023-11-07T18:52:02.671090Z",
          "iopub.status.idle": "2023-11-07T18:52:02.675231Z",
          "shell.execute_reply": "2023-11-07T18:52:02.674616Z",
          "shell.execute_reply.started": "2023-11-07T18:52:02.671291Z"
        },
        "id": "bca54146-fa57-4132-81ee-d09ee6e8c8b8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def index_chars(sentences):\n",
        "    megasentence = ''.join(sentences)\n",
        "    char_list = set()\n",
        "    for c in megasentence:\n",
        "        char_list.add(c)\n",
        "    char_list = list(char_list)\n",
        "    return char_list, {char_list[x]:x for x in range(len(char_list))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ee747721-c64f-4bef-a3e6-179b0b7b8cf1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:02.679530Z",
          "iopub.status.busy": "2023-11-07T18:52:02.679270Z",
          "iopub.status.idle": "2023-11-07T18:52:02.699628Z",
          "shell.execute_reply": "2023-11-07T18:52:02.699062Z",
          "shell.execute_reply.started": "2023-11-07T18:52:02.679511Z"
        },
        "id": "ee747721-c64f-4bef-a3e6-179b0b7b8cf1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "int_index, char_index = index_chars([x[0] for x in train_sentences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "436934e4-f09d-4c50-89ac-6f3507109c0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:02.700590Z",
          "iopub.status.busy": "2023-11-07T18:52:02.700384Z",
          "iopub.status.idle": "2023-11-07T18:52:02.706845Z",
          "shell.execute_reply": "2023-11-07T18:52:02.706282Z",
          "shell.execute_reply.started": "2023-11-07T18:52:02.700572Z"
        },
        "id": "436934e4-f09d-4c50-89ac-6f3507109c0b",
        "outputId": "c9b4a109-f15c-43fd-b86c-5dd7cf7fa7ec",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('玖', 40)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "int_index[40], char_index[int_index[40]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "49296943-759d-42b2-a19b-941ad0e4bf8f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:56.011095Z",
          "iopub.status.busy": "2023-11-07T18:52:56.010726Z",
          "iopub.status.idle": "2023-11-07T18:52:56.017448Z",
          "shell.execute_reply": "2023-11-07T18:52:56.016752Z",
          "shell.execute_reply.started": "2023-11-07T18:52:56.011070Z"
        },
        "id": "49296943-759d-42b2-a19b-941ad0e4bf8f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Chinese_Dataset(Dataset):\n",
        "    def __init__(self, sequences, tokenizer):\n",
        "        self.sequences = [x[0] for x in sequences]\n",
        "        self.ner_tags = [x[1] for x in sequences]\n",
        "        int_index, char_index = index_chars([x for x in self.sequences])\n",
        "        self.int_indices = int_index\n",
        "        self.char_indices = char_index\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(self.sequences[idx], padding='max_length', truncation=True, max_length=182, return_tensors='pt')\n",
        "\n",
        "        max_seq_length = len(encoding['input_ids'][0])\n",
        "        labels = self.ner_tags[idx]\n",
        "        padded_labels = labels + [0] * (max_seq_length - len(labels))\n",
        "\n",
        "        item = {\n",
        "            'tokens': self.sequences[idx],\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            #'attention_mask': encoding['attention_mask'],\n",
        "            'labels': padded_labels,\n",
        "        }\n",
        "        #item = {\"id\": idx, \"tokens\": self.sequences[idx], \"tags\": self.ner_tags[idx]}\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9a9e29ab-a68e-4842-89dd-7ef0c15aa2e9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:58.462989Z",
          "iopub.status.busy": "2023-11-07T18:52:58.462669Z",
          "iopub.status.idle": "2023-11-07T18:52:58.519292Z",
          "shell.execute_reply": "2023-11-07T18:52:58.518695Z",
          "shell.execute_reply.started": "2023-11-07T18:52:58.462969Z"
        },
        "id": "9a9e29ab-a68e-4842-89dd-7ef0c15aa2e9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f46eb1bd-d0e6-4de3-8951-f1cc58ae6d3e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:58.816567Z",
          "iopub.status.busy": "2023-11-07T18:52:58.816058Z",
          "iopub.status.idle": "2023-11-07T18:52:58.838325Z",
          "shell.execute_reply": "2023-11-07T18:52:58.837787Z",
          "shell.execute_reply.started": "2023-11-07T18:52:58.816536Z"
        },
        "id": "f46eb1bd-d0e6-4de3-8951-f1cc58ae6d3e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data = Chinese_Dataset(train_sentences, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5b933ed8-2266-4e7c-aeaf-604282c37aa7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:59.053418Z",
          "iopub.status.busy": "2023-11-07T18:52:59.052898Z",
          "iopub.status.idle": "2023-11-07T18:52:59.059801Z",
          "shell.execute_reply": "2023-11-07T18:52:59.058903Z",
          "shell.execute_reply.started": "2023-11-07T18:52:59.053388Z"
        },
        "id": "5b933ed8-2266-4e7c-aeaf-604282c37aa7",
        "outputId": "b8d1315c-78a7-486e-98de-3799aae4ee42",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Length: 3997\n",
            "Sample Item: {'tokens': '現存的三棟屋村位於三棟屋路近和宜合交匯處一帶。', 'input_ids': tensor([ 101, 4412, 2100, 4638,  676, 3477, 2238, 3333,  855, 3176,  676, 3477,\n",
            "        2238, 6662, 6818, 1469, 2139, 1394,  769, 1274, 5993,  671, 2380,  511,\n",
            "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0]), 'labels': [1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "#checking if everything is working\n",
        "print(\"Dataset Length:\", len(data))\n",
        "sample_item = data[40]\n",
        "print(\"Sample Item:\", sample_item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0fca22d6-c6ad-4a5c-a605-19d4360be07b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:59.312661Z",
          "iopub.status.busy": "2023-11-07T18:52:59.312160Z",
          "iopub.status.idle": "2023-11-07T18:52:59.316797Z",
          "shell.execute_reply": "2023-11-07T18:52:59.316095Z",
          "shell.execute_reply.started": "2023-11-07T18:52:59.312640Z"
        },
        "id": "0fca22d6-c6ad-4a5c-a605-19d4360be07b",
        "outputId": "9e9ed0ee-a112-40af-a48d-372b30654f37",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('玖', 40)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "data.int_indices[40], data.char_indices[int_index[40]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "08c15de9-14fa-44a0-80dc-ef593f44858e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:59.512847Z",
          "iopub.status.busy": "2023-11-07T18:52:59.512595Z",
          "iopub.status.idle": "2023-11-07T18:52:59.518631Z",
          "shell.execute_reply": "2023-11-07T18:52:59.517867Z",
          "shell.execute_reply.started": "2023-11-07T18:52:59.512829Z"
        },
        "id": "08c15de9-14fa-44a0-80dc-ef593f44858e",
        "outputId": "1a769250-2de9-4075-8626-0dc19aa11d80",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " '現',\n",
              " '存',\n",
              " '的',\n",
              " '三',\n",
              " '棟',\n",
              " '屋',\n",
              " '村',\n",
              " '位',\n",
              " '於',\n",
              " '三',\n",
              " '棟',\n",
              " '屋',\n",
              " '路',\n",
              " '近',\n",
              " '和',\n",
              " '宜',\n",
              " '合',\n",
              " '交',\n",
              " '匯',\n",
              " '處',\n",
              " '一',\n",
              " '帶',\n",
              " '。',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "example = data[40]\n",
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=False)\n",
        "# Convert token IDs to tokens (words)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8f2c5d02-b335-4522-b993-f26078c5e830",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:59.691985Z",
          "iopub.status.busy": "2023-11-07T18:52:59.691440Z",
          "iopub.status.idle": "2023-11-07T18:52:59.695052Z",
          "shell.execute_reply": "2023-11-07T18:52:59.694286Z",
          "shell.execute_reply.started": "2023-11-07T18:52:59.691964Z"
        },
        "id": "8f2c5d02-b335-4522-b993-f26078c5e830",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "58a8aec6-18eb-4154-b9b1-963465c0694b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:52:59.883090Z",
          "iopub.status.busy": "2023-11-07T18:52:59.882840Z",
          "iopub.status.idle": "2023-11-07T18:52:59.887015Z",
          "shell.execute_reply": "2023-11-07T18:52:59.886431Z",
          "shell.execute_reply.started": "2023-11-07T18:52:59.883072Z"
        },
        "id": "58a8aec6-18eb-4154-b9b1-963465c0694b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def assign_labels(sentences):\n",
        "    labels_list = []\n",
        "    for i in range(len(sentences)):\n",
        "        labels = []\n",
        "        tags = sentences[i][\"labels\"]\n",
        "        for tag in tags:\n",
        "            sentence_labels = []\n",
        "            if tag == 1:\n",
        "                labels.append(\"beginning_of_word\")\n",
        "            else:\n",
        "                labels.append(\"continuation_of_word\")\n",
        "        labels_list.append(labels)\n",
        "    return labels_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7ab22ff6-b245-42e4-a87a-fcfb29b4cf21",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:00.258971Z",
          "iopub.status.busy": "2023-11-07T18:53:00.258724Z",
          "iopub.status.idle": "2023-11-07T18:53:01.228711Z",
          "shell.execute_reply": "2023-11-07T18:53:01.228102Z",
          "shell.execute_reply.started": "2023-11-07T18:53:00.258954Z"
        },
        "id": "7ab22ff6-b245-42e4-a87a-fcfb29b4cf21",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "id2labels = assign_labels(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "88de5c41-f571-437f-9f14-36201fe16539",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:01.231038Z",
          "iopub.status.busy": "2023-11-07T18:53:01.230535Z",
          "iopub.status.idle": "2023-11-07T18:53:01.428077Z",
          "shell.execute_reply": "2023-11-07T18:53:01.427561Z",
          "shell.execute_reply.started": "2023-11-07T18:53:01.231004Z"
        },
        "id": "88de5c41-f571-437f-9f14-36201fe16539",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install seqeval\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "labels = [id2labels[i] for i in example[f\"labels\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "871683e1-c5e3-45a0-be42-72cb945b71bd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:01.429106Z",
          "iopub.status.busy": "2023-11-07T18:53:01.428843Z",
          "iopub.status.idle": "2023-11-07T18:53:01.434414Z",
          "shell.execute_reply": "2023-11-07T18:53:01.433671Z",
          "shell.execute_reply.started": "2023-11-07T18:53:01.429086Z"
        },
        "id": "871683e1-c5e3-45a0-be42-72cb945b71bd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fb99d3cb-49cf-4003-937e-f0bdbedad881",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:01.435941Z",
          "iopub.status.busy": "2023-11-07T18:53:01.435672Z",
          "iopub.status.idle": "2023-11-07T18:53:01.439486Z",
          "shell.execute_reply": "2023-11-07T18:53:01.438753Z",
          "shell.execute_reply.started": "2023-11-07T18:53:01.435923Z"
        },
        "id": "fb99d3cb-49cf-4003-937e-f0bdbedad881",
        "tags": []
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "    1: \"beginning_of_word\",\n",
        "    0: \"continuation_of_word\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f5dccc6a-badb-4dc3-88f5-425a5ca405ac",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:01.440826Z",
          "iopub.status.busy": "2023-11-07T18:53:01.440484Z",
          "iopub.status.idle": "2023-11-07T18:53:01.443907Z",
          "shell.execute_reply": "2023-11-07T18:53:01.443357Z",
          "shell.execute_reply.started": "2023-11-07T18:53:01.440796Z"
        },
        "id": "f5dccc6a-badb-4dc3-88f5-425a5ca405ac",
        "tags": []
      },
      "outputs": [],
      "source": [
        "label2id = {\n",
        "    \"beginning_of_word\": 1,\n",
        "    \"continuation_of_word\": 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ca1e5e86-f0c7-4c77-bd42-5cdf541a83ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:01.484536Z",
          "iopub.status.busy": "2023-11-07T18:53:01.484058Z",
          "iopub.status.idle": "2023-11-07T18:53:03.441457Z",
          "shell.execute_reply": "2023-11-07T18:53:03.440780Z",
          "shell.execute_reply.started": "2023-11-07T18:53:01.484515Z"
        },
        "id": "ca1e5e86-f0c7-4c77-bd42-5cdf541a83ce",
        "outputId": "87e89858-7d74-447a-f2ea-2130bf85b72e",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "chinese_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\", num_labels=2, id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d60c6697-8f97-437b-b64e-c7babac7de69",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:03.443641Z",
          "iopub.status.busy": "2023-11-07T18:53:03.443114Z",
          "iopub.status.idle": "2023-11-07T18:53:03.447500Z",
          "shell.execute_reply": "2023-11-07T18:53:03.446540Z",
          "shell.execute_reply.started": "2023-11-07T18:53:03.443606Z"
        },
        "id": "d60c6697-8f97-437b-b64e-c7babac7de69",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "#chinese_model.to(device)\n",
        "#model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0f7ff1c8-169b-4a4f-95e2-ea831e82b2a5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:03.449004Z",
          "iopub.status.busy": "2023-11-07T18:53:03.448648Z",
          "iopub.status.idle": "2023-11-07T18:53:03.453344Z",
          "shell.execute_reply": "2023-11-07T18:53:03.452503Z",
          "shell.execute_reply.started": "2023-11-07T18:53:03.448974Z"
        },
        "id": "0f7ff1c8-169b-4a4f-95e2-ea831e82b2a5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "training_steps = (2 * len(data))/16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b731c078-d98a-46ae-b073-8b97334f6d76",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:03.455840Z",
          "iopub.status.busy": "2023-11-07T18:53:03.455077Z",
          "iopub.status.idle": "2023-11-07T18:53:03.462308Z",
          "shell.execute_reply": "2023-11-07T18:53:03.461722Z",
          "shell.execute_reply.started": "2023-11-07T18:53:03.455813Z"
        },
        "id": "b731c078-d98a-46ae-b073-8b97334f6d76",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install transformers[torch]\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"BERT_and_chinese\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"no\",\n",
        "    load_best_model_at_end=False,\n",
        "    push_to_hub=False,\n",
        "    #measure to avoid cuda errors with Trainer when training chinese BERT\n",
        "    eval_accumulation_steps=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "GsnIkxDrDjsO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsnIkxDrDjsO",
        "outputId": "3609b2a2-3ca3-45d1-9537-d79c11c08c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3197 800 3997\n"
          ]
        }
      ],
      "source": [
        "train_dataset = int(0.8*len(data))\n",
        "validation_dataset = len(data) - train_dataset\n",
        "\n",
        "train, validation = random_split(data, (train_dataset, validation_dataset))\n",
        "print(len(train), len(validation), len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "28a59316-b81f-4966-bad7-6c42037a30b4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:05.377031Z",
          "iopub.status.busy": "2023-11-07T18:53:05.376571Z",
          "iopub.status.idle": "2023-11-07T18:53:05.453851Z",
          "shell.execute_reply": "2023-11-07T18:53:05.453299Z",
          "shell.execute_reply.started": "2023-11-07T18:53:05.377010Z"
        },
        "id": "28a59316-b81f-4966-bad7-6c42037a30b4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=chinese_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=validation,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train,\n",
        "#     eval_dataset=validation,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator=data_collator,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e284b674-06d4-4e16-bac2-7beaf17a4a41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "execution": {
          "iopub.execute_input": "2023-11-07T18:53:05.992596Z",
          "iopub.status.busy": "2023-11-07T18:53:05.992318Z"
        },
        "id": "e284b674-06d4-4e16-bac2-7beaf17a4a41",
        "outputId": "635cfb28-02bf-4fb3-dd9e-1aa633f683d2",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [400/400 04:10, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d0f7e039-484a-4451-8461-196ee5eaba7f",
      "metadata": {
        "id": "d0f7e039-484a-4451-8461-196ee5eaba7f"
      },
      "outputs": [],
      "source": [
        "#fine tuning\n",
        "\n",
        "from transformers import create_optimizer, pipeline\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "66b6c3e9-225e-474b-b2c1-83ff22fbc510",
      "metadata": {
        "id": "66b6c3e9-225e-474b-b2c1-83ff22fbc510"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "num_train_epochs = 3\n",
        "num_train_steps = (len(data)// batch_size) * num_train_epochs\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        "    num_warmup_steps=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "979e472b-7cd6-4708-86b0-e2fdc07fa4d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "979e472b-7cd6-4708-86b0-e2fdc07fa4d0",
        "outputId": "30b505a8-1630-4b22-f63b-8ec13d9dd467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForTokenClassification\n",
        "\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "chinese_model = TFAutoModelForTokenClassification.from_pretrained(\"bert-base-chinese\", num_labels=2, id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1jWeQb-447fp",
      "metadata": {
        "id": "1jWeQb-447fp"
      },
      "outputs": [],
      "source": [
        "def transform_dataset(dataset):\n",
        "  data = {}\n",
        "  for i in range(len(dataset)):\n",
        "    item = dataset[i]\n",
        "    input_ids = item[\"input_ids\"]\n",
        "    labels = item[\"labels\"]\n",
        "    data[input_ids] = labels\n",
        "  format = {\"input_ids\" : list(data.keys()), \"labels\" : list(data.values())}\n",
        "  dataset = ds.from_dict(format)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "_VQ-bxXG8OQt",
      "metadata": {
        "id": "_VQ-bxXG8OQt"
      },
      "outputs": [],
      "source": [
        "train_dataset = transform_dataset(train)\n",
        "validation_dataset = transform_dataset(validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "Ter3ouDw-j46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ter3ouDw-j46",
        "outputId": "69587def-a95a-443e-ea58-f4877a8e2456"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'labels'],\n",
              "    num_rows: 3197\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "R-9cnvLh8h0b",
      "metadata": {
        "id": "R-9cnvLh8h0b"
      },
      "outputs": [],
      "source": [
        "chinese_dataset = DatasetDict({\"train\":train_dataset, \"test\":validation_dataset})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "FMHZOu6gBtL0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMHZOu6gBtL0",
        "outputId": "8767788f-70b2-48ae-bb77-c67f42bccd43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'labels'],\n",
              "        num_rows: 3197\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'labels'],\n",
              "        num_rows: 800\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "chinese_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "12f3345c-2f44-436b-85fc-504b17ce7ad2",
      "metadata": {
        "id": "12f3345c-2f44-436b-85fc-504b17ce7ad2"
      },
      "outputs": [],
      "source": [
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    chinese_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = model.prepare_tf_dataset(\n",
        "    chinese_dataset[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "66840ff7-6faf-40a9-a3d1-8240044973c3",
      "metadata": {
        "id": "66840ff7-6faf-40a9-a3d1-8240044973c3"
      },
      "outputs": [],
      "source": [
        "chinese_model.compile(optimizer=optimizer)\n",
        "#model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b5038a85-957b-49d2-a14c-aa7109b9ec12",
      "metadata": {
        "id": "b5038a85-957b-49d2-a14c-aa7109b9ec12"
      },
      "outputs": [],
      "source": [
        "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a072e532-82f6-4151-a204-6987a4183ff6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a072e532-82f6-4151-a204-6987a4183ff6",
        "outputId": "c99be653-f6d6-452a-91d0-770f322169c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "199/199 [==============================] - ETA: 0s - loss: 0.1158"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ['beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word'] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ['beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'beginning_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word', 'continuation_of_word'] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r199/199 [==============================] - 218s 1s/step - loss: 0.1158 - val_loss: 0.0776 - precision: 0.7531 - recall: 0.6370 - f1: 0.6902 - accuracy: 0.9615\n",
            "Epoch 2/3\n",
            "199/199 [==============================] - 196s 987ms/step - loss: 0.0683 - val_loss: 0.0677 - precision: 0.8434 - recall: 0.6430 - f1: 0.7297 - accuracy: 0.9658\n",
            "Epoch 3/3\n",
            "199/199 [==============================] - 207s 1s/step - loss: 0.0601 - val_loss: 0.0644 - precision: 0.8421 - recall: 0.6502 - f1: 0.7338 - accuracy: 0.9669\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d36a068fb50>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "chinese_model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=metric_callback)\n",
        "#model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=metric_callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "2531060a-dbfe-4580-a303-0393c5990b57",
      "metadata": {
        "id": "2531060a-dbfe-4580-a303-0393c5990b57"
      },
      "outputs": [],
      "source": [
        "text = test_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d299wsTRxNQe",
        "outputId": "fdeb180a-c0b0-4cd8-b1fe-7f26bd0cc4c0"
      },
      "id": "d299wsTRxNQe",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('然而，這樣的處理也衍生了一些問題。', [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "34d2c5a1-41cc-4e0a-9e5f-dd799ce899ca",
      "metadata": {
        "id": "34d2c5a1-41cc-4e0a-9e5f-dd799ce899ca"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e59ff2b2-1a32-42a5-9a16-7db1f0bf5e2b",
      "metadata": {
        "id": "e59ff2b2-1a32-42a5-9a16-7db1f0bf5e2b"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"BERT_and_chinese\")\n",
        "inputs = tokenizer(text[0], return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "5c54a320-a473-48e3-8144-ea0629eb5404",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c54a320-a473-48e3-8144-ea0629eb5404",
        "outputId": "fe82e774-c049-44f7-d23f-22c91dde2dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at BERT_and_chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"BERT_and_chinese\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "a9c4f674-4931-4f70-9f79-9c490d39cc71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9c4f674-4931-4f70-9f79-9c490d39cc71",
        "outputId": "ffd6ae88-4fad-4783-de67-3e7223c7407d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'beginning_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'beginning_of_word',\n",
              " 'continuation_of_word',\n",
              " 'beginning_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word',\n",
              " 'continuation_of_word']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "#Gets the class with the highest probability, and uses the model’s id2label mapping to convert it to a text label:\n",
        "\n",
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "predicted_token_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e50d0f9b-68ca-49e1-adc4-8c31c5fe6a9f",
      "metadata": {
        "id": "e50d0f9b-68ca-49e1-adc4-8c31c5fe6a9f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}